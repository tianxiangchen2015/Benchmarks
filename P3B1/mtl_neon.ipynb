{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle, gzip\n",
    "\n",
    "from neon import logger as neon_logger\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "from neon.data import MNIST\n",
    "from neon.initializers import Gaussian, GlorotUniform\n",
    "from neon.layers import Affine, BranchNode, Multicost, Tree, GeneralizedCost, SingleOutputTree, MergeMultistream\n",
    "from neon.models import Model\n",
    "from neon.optimizers import RMSProp\n",
    "from neon.transforms import Rectlin, Logistic, Softmax\n",
    "from neon.transforms import CrossEntropyMulti, Misclassification, PrecisionRecall\n",
    "from neon.util.argparser import NeonArgparser\n",
    "from neon.data import ArrayIterator\n",
    "from neon.backends import gen_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "be = gen_backend(backend='cpu',batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.data import NervanaDataIterator\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import os\n",
    "\n",
    "class SVHN(NervanaDataIterator):\n",
    "\n",
    "    def __init__(self, X, Y, nclass):\n",
    "        \n",
    "        def onehot_gen(z, s, n):\n",
    "            b = np.zeros((s, n))\n",
    "            b[np.arange(s), z] = 1\n",
    "            return b\n",
    "        # Load the numpy data into some variables. We divide the image by 255 to normalize the values\n",
    "        # between 0 and 1.\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.nclass_t1 = nclass[0]\n",
    "        self.nclass_t2 = nclass[1]\n",
    "        self.nclass_t3 = nclass[2]\n",
    "        self.shape = [x.shape[1] for x in X]\n",
    "#         self.shape = lshape  # shape of the input data (e.g. for images, (C, H, W))\n",
    "\n",
    "        # 1. assign some required and useful attributes\n",
    "        self.start = 0  # start at zero\n",
    "        self.ndata = self.X[0].shape[0]  # number of images in X (hint: use X.shape)\n",
    "        self.nfeatures = self.X[0].shape[1]  # number of features in X (hint: use X.shape)\n",
    "        self.Y_t1 = onehot_gen(self.Y[0], self.ndata, self.nclass_t1)\n",
    "        self.Y_t2 = onehot_gen(self.Y[1], self.ndata, self.nclass_t2)\n",
    "        self.Y_t3 = onehot_gen(self.Y[2], self.ndata, self.nclass_t3)\n",
    "\n",
    "        # number of minibatches per epoch\n",
    "        # to calculate this, use the batchsize, which is stored in self.be.bsz\n",
    "        self.nbatches = self.ndata/self.be.bsz \n",
    "        \n",
    "        \n",
    "        # 2. allocate memory on the CPU for a minibatch's worth of data.\n",
    "        # (e.g. use `self.be` to access the backend.). See the backend documentation.\n",
    "        # to get the minibatch size, use self.be.bsz\n",
    "        # hint: X should have shape (# features, mini-batch size)\n",
    "        # hint: use some of the attributes previously defined above\n",
    "        self.dev_X_t1 = self.be.zeros((self.nfeatures, self.be.bsz))\n",
    "        self.dev_X_t2 = self.be.zeros((self.nfeatures, self.be.bsz))\n",
    "        self.dev_X_t3 = self.be.zeros((self.nfeatures, self.be.bsz))\n",
    "        self.dev_Y_t1 = self.be.zeros((self.Y_t1.shape[1], self.be.bsz))\n",
    "        self.dev_Y_t2 = self.be.zeros((self.Y_t2.shape[1], self.be.bsz))\n",
    "        self.dev_Y_t3 = self.be.zeros((self.Y_t3.shape[1], self.be.bsz))\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "           \n",
    "    def __iter__(self):\n",
    "        # 3. loop through minibatches in the dataset\n",
    "        for index in range(self.start, self.ndata, self.be.bsz):\n",
    "            # 3a. grab the right slice from the numpy arrays\n",
    "            inputs_t1 = self.X[0][index:(index + self.be.bsz), :]\n",
    "            targets_t1 = self.Y_t1[index:(index + self.be.bsz), :]\n",
    "            inputs_t2 = self.X[1][index:(index + self.be.bsz), :]\n",
    "            targets_t2 = self.Y_t2[index:(index + self.be.bsz), :]\n",
    "            inputs_t3 = self.X[2][index:(index + self.be.bsz), :]\n",
    "            targets_t3 = self.Y_t3[index:(index + self.be.bsz), :]\n",
    "            \n",
    "            \n",
    "            # The arrays X and Y data are in shape (batch_size, num_features),\n",
    "            # but the iterator needs to return data with shape (num_features, batch_size).\n",
    "            # here we transpose the data, and then store it as a contiguous array. \n",
    "            # numpy arrays need to be contiguous before being loaded onto the GPU.\n",
    "            inputs_t1 = np.ascontiguousarray(inputs_t1.T)\n",
    "            targets_t1 = np.ascontiguousarray(targets_t1.T)\n",
    "            inputs_t2 = np.ascontiguousarray(inputs_t2.T)\n",
    "            targets_t2 = np.ascontiguousarray(targets_t2.T)\n",
    "            inputs_t3 = np.ascontiguousarray(inputs_t3.T)\n",
    "            targets_t3 = np.ascontiguousarray(targets_t3.T)\n",
    "                        \n",
    "            # here we test your implementation\n",
    "            # your slice has to have the same shape as the GPU tensors you allocated\n",
    "#             assert inputs[0].shape == self.dev_X_t1.shape, \\\n",
    "#                    \"inputs has shape {}, but dev_X is {}\".format(inputs.shape, self.dev_X.shape)\n",
    "#             assert targets[0].shape == self.dev_Y_t1.shape, \\\n",
    "#                    \"targets has shape {}, but dev_Y is {}\".format(targets.shape, self.dev_Y.shape)\n",
    "            \n",
    "            # 3b. transfer from numpy arrays to device\n",
    "            # - use the GPU memory buffers allocated previously,\n",
    "            #    and call the myTensorBuffer.set() function. \n",
    "            self.dev_X_t1.set(inputs_t1)\n",
    "            self.dev_Y_t1.set(targets_t1)\n",
    "            self.dev_X_t2.set(inputs_t2)\n",
    "            self.dev_Y_t2.set(targets_t2)\n",
    "            self.dev_X_t3.set(inputs_t3)\n",
    "            self.dev_Y_t3.set(targets_t3)\n",
    "            \n",
    "            # 3c. yield a tuple of the device tensors.\n",
    "            # X should be of shape (num_features, batch_size)\n",
    "            # Y should be of shape (4, batch_size)\n",
    "            yield ((self.dev_X_t1, self.dev_X_t2, self.dev_X_t3 ), (self.dev_Y_t1, self.dev_Y_t2, self.dev_Y_t3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a number of initial variables for use with baseline\n",
    "NUM_TASKS = 3; # number of learning tasks (for multi-task learning)\n",
    "NUM_FOLDS = 10; # number of folds for training (main cross validation loop)\n",
    "NUM_EPOCH = 5; # number of epochs\n",
    "\n",
    "\n",
    "\n",
    "truth_a_arr = []\n",
    "pred_a_arr = []\n",
    "\n",
    "truth_b_arr = []\n",
    "pred_b_arr = []\n",
    "\n",
    "truth_c_arr = []\n",
    "pred_c_arr = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fold in range( NUM_FOLDS ):\n",
    "\n",
    "    features_train = []\n",
    "    labels_train = []\n",
    "    truths_train = []\n",
    "\n",
    "    features_test = []\n",
    "    labels_test = []\n",
    "    truths_test = []\n",
    "\n",
    "    n_out = []\n",
    "\n",
    "    for task in range( NUM_TASKS ):\n",
    "        file_post = '.' + str(task) + '.' + str(fold) + '.pkl.gz'\n",
    "        fname_train = 'train/train' + file_post; \n",
    "        fname_test  = 'test/test' + file_post; \n",
    "        with gzip.open( fname_train, 'rb' ) as f:\n",
    "            feature_train, label_train = cPickle.load( f )\n",
    "\n",
    "        with gzip.open( fname_test, 'rb') as f:\n",
    "            feature_test, label_test = cPickle.load( f )\n",
    "\n",
    "        features_train.append( feature_train )\n",
    "        labels_train.append( label_train )\n",
    "\n",
    "        features_test.append( feature_test )\n",
    "        labels_test.append( label_test )\n",
    "\n",
    "        mv = np.max( label_train )\n",
    "        truth_train = np.zeros( ( len( label_train ), mv + 1 ) )\n",
    "        for i in range( len( label_train ) ):\n",
    "            truth_train[ i, label_train[ i ] ] = 1\n",
    "\n",
    "        truths_train.append( truth_train )\n",
    "\n",
    "        mv = np.max( label_test )\n",
    "        truth_test = np.zeros( ( len( label_test ), mv + 1 ) )\n",
    "        for i in range( len( label_test ) ):\n",
    "            truth_test[ i, label_test[ i ] ] = 1\n",
    "\n",
    "        truths_test.append( truth_test )\n",
    "\n",
    "        n_out.append( mv + 1 )\n",
    "\n",
    "    flen = len( feature_train[ 0 ] ); # input feature length is set to 400 for now based on the training examples available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_labels = [array[0:45] for array in labels_test]\n",
    "test_features = [array[0:45] for array in features_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = SVHN(X = features_train, Y = labels_train, nclass=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = SVHN(X = test_features, Y = test_labels, nclass=n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = BranchNode(name='b1')\n",
    "init = GlorotUniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1_input = [Affine(nout=flen, name=\"t1_input\", activation=Rectlin(), init=init)]\n",
    "t2_input = [Affine(nout=flen, name=\"t2_input\", activation=Rectlin(), init=init)]\n",
    "t3_input = [Affine(nout=flen, name=\"t3_input\", activation=Rectlin(), init=init)]\n",
    "p1 = [MergeMultistream(layers = [t1_input, t2_input, t3_input], merge='stack'),\n",
    "         Affine(nout=flen, name=\"share\", activation=Rectlin(), init=init),\n",
    "         b1,\n",
    "         Affine(nout=flen, name=\"t1_3\", activation=Rectlin(), init=init),\n",
    "         Affine(nout=256, name=\"t1_4\", activation=Rectlin(), init=init),\n",
    "         Affine(nout=n_out[0], name=\"t2_out\", activation=Rectlin(), init=init)]\n",
    "\n",
    "p2 = [b1,\n",
    "      Affine(nout=flen, name=\"t2_3\", activation=Rectlin(), init=init),\n",
    "      Affine(nout=256, name=\"t2_4\", activation=Rectlin(), init=init),\n",
    "      Affine(nout=n_out[1], name=\"t2_out\", activation=Rectlin(), init=init)]\n",
    "p3 = [b1,\n",
    "      Affine(nout=flen, name=\"t3_3\", activation=Rectlin(), init=init),\n",
    "      Affine(nout=256, name=\"t3_4\", activation=Rectlin(), init=init),\n",
    "      Affine(nout=n_out[2], name=\"t3_out\", activation=Rectlin(), init=init)]\n",
    "model = Model(layers=Tree([p1, p2, p3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = RMSProp(learning_rate=0.001)\n",
    "cost = Multicost(costs=[GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True)),\n",
    "                       GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True)),\n",
    "                       GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))])\n",
    "                        \n",
    "# callbacks_1 = Callbacks(model_1, eval_set=test_t1, eval_freq=NUM_EPOCH)\n",
    "# callbacks_2 = Callbacks(model_2, eval_set=test_t2, eval_freq=NUM_EPOCH)\n",
    "# callbacks_3 = Callbacks(model_3, eval_set=test_t3, eval_freq=NUM_EPOCH)\n",
    "\n",
    "callbacks_1 = Callbacks(model, eval_set=train, eval_freq=NUM_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost1 = CrossEntropyMulti(usebits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |████████████████████|  120/120  batches, (72.13, 72.13, 72.13) costs, 3.47s]\n",
      "Epoch 1   [Train |████████████████████|  120/120  batches, (72.13, 72.13, 72.13) costs, 3.50s]\n",
      "Epoch 2   [Train |████████████████████|  120/120  batches, (0.01, 72.13, 72.13) costs, 3.52s]\n",
      "Epoch 3   [Train |████████████████████|  120/120  batches, (0.01, 72.13, 0.02) costs, 3.50s]\n",
      "Epoch 4   [Train |████████████████████|  120/120  batches, (0.01, 72.13, 0.01) costs, 3.48s] [CrossEntropyMulti Loss 36.01, 0.52s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train, optimizer=optimizer, cost=cost, callbacks=callbacks_1, \n",
    "            num_epochs=NUM_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-6d3629f5fc4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneralizedCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcostfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCrossEntropyMulti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musebits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/neon-1.8.2-py2.7.egg/neon/models/model.pyc\u001b[0m in \u001b[0;36mget_outputs\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mYpred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mdim0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mYpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mnsteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim1\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "testcost = GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))\n",
    "model.get_outputs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
